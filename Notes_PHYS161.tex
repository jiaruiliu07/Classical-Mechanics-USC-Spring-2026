\documentclass[10pt]{article}
\usepackage{NotesTeXV3,lipsum}
\usepackage{lmodern}
\usepackage{fontspec}
\usepackage{physics}

% Physics note color palette
\definecolor{physgreen}{RGB}{34,139,34}
\definecolor{physpurple}{RGB}{128,0,128}
\definecolor{physblue}{RGB}{30,144,255}
\definecolor{physorange}{RGB}{255,140,0}
\definecolor{physgray}{RGB}{80,80,80}

\setmainfont{Source Serif Pro}


\tolerance=10000
\emergencystretch=3em
\hbadness=10000
\hfuzz=2pt


\begin{document}
	\title{{Advanced Principles of Mechanics I}\\{\normalsize{Notes on USC Spring 2026 Course PHYS-161}}}
	\author{Jiarui Liu}
	\affiliation{
	High School Student at Nanjing Foreign Language School\\
	\href{https://conversionindex.org}{Website}\\
	\href{https://www.linkedin.com/in/jiarui-liu-994007294/}{LinkedIn}\\
	\href{https://github.com/jiaruiliu07}{GitHub}\\
	}
	\emailAdd{jliu1978@usc.edu}
	\maketitle
	\newpage
	\pagestyle{fancynotes}
	\part{Special Relativity}

	\section{Dimensions and Dimensional Analysis}
	\textit{This will be an experimental course: starting with the concept of space-time, going back to kinematics, then relativistic dynamics, lastly energy.}
	
	\vspace{1\baselineskip}
	\noindent Definition of physics:
	\begin{definition}
		Physics is the natural quantitative science that deals with the fundamental interactions between matter and energy.
	\end{definition}

	What arguably sets physics apart from the other sciences would be the reductionist nature. At the base of the "pyramid of the natural sciences" lies mathematics, the fundamental subjects. Up a level is Physics, then Chemistry, then Biology. Because it's quantitative, when we study the world around us with measurements and standardizations. This introduces us to \textit{dimensions and unit}:
	\begin{claim}
		Nearly all properties in classical physics can be described with one or more of the three dimensions: length, time, and mass.
	\end{claim}

	The notation would be $[x]$ as the dimension of some variable or constant, for example
	\begin{equation}
	  [x] = L.
	\end{equation}

	The SI unit is the international unit. Remember that the standard for mass in SI is kilograms. Now we introduce the natural unit. 

	\begin{definition}
		The natural unit is defined by setting the values of the three constants:
		\begin{equation}
	  	  c = \hbar = G = 1.
		\end{equation}
	\end{definition}

	Here they still have dimensions, for example:
	\begin{equation}
	  [c] = \frac{L}{T} = 1 \quad \Rightarrow \quad L = T.
	\end{equation}

	\noindent \textbf{Dimensional Analysis}

	\vspace{1\baselineskip}
	A simple pendulum on Earth has length $L$, mass $m$. Given
	\begin{equation}
	  [l] = L, [m] = M, \;\text{and}\; [g] = \frac{L}{T^{2}}
	\end{equation}

	determine, up to dimensionless factors, the period of the pendulum.

	\vspace{1\baselineskip}
	\textbf{Solution.} We have, apparently, the period $\tau$ with dimension $T$. We would assume the powers with greek letters:
	\begin{equation}
	  [\tau] = T = [l^{\alpha} m^{\beta} g^{\gamma}] = L^{\alpha} M^{\beta} \left(\frac{L}{T^{2}}\right)^{\gamma}.
	\end{equation}

	This is solved simply as:
	\begin{equation}
	  \begin{cases} \alpha + \gamma = 0, \\ \beta = 0, \\ -2\gamma = 1 \end{cases} \quad \Rightarrow \quad \begin{cases} \alpha = \frac{1}{2}, \\ \beta = 0, \\ \gamma = -\frac{1}{2} \end{cases}.
	\end{equation}

	Which gives us
	\begin{equation}
	  \tau \propto l^{\frac{1}{2}} g^{-\frac{1}{2}} = \sqrt{\frac{l}{g}}
	\end{equation}

	\begin{exercise}
		Look at the dimensions of $c,\hbar, \,\text{and} \, G$. Create the Planck units- the Planck length, time, and mass, by combining the three constants in a unique way that they create a constant for length, time, and mass.
	\end{exercise}

	
	\section{Definitions}
	We start by defining the basics of kinematics- the study of "how things move". The simplest case of motion is none- an object being stationary. Now there will be a question: where is it. Now we define the frame of reference and point particles. 
	\begin{definition}
		A frame of reference for an observer is a sufficiently precise laboratory of measurement approaches, i.e. clocks, meter sticks. Here, in classical physics, we assume a precise measurement without quantum fluctuation, and it can be arbitrarily precise.
	\end{definition}

	And we define the point particle:
	\begin{definition}
		A point particle is an approximation whereby an object is treated as a zero-dimensional, mathematical point. It is valid when the size and shape of the object are irrelevant for the question under study.
	\end{definition}

	In this frame of reference, we may set up Cartesian coordinates for space $(x,y,z)$ to uniquely specify the position of a particle. The Cartesian coordinate is defined by three everywhere mutually perpendicular axes intersecting at the same point, defined as the origin. 
	
	\vspace{\baselineskip}
	Now it's the problem of incorporating time: the sequence of locations a particle passes can be described as time. It cannot, to our best knowledge be reversed, so it's the monotonically increasing parameter denoting the particle's position. We treat it as another label on the event: write $(ct,x,y,z)$. Thus, to locate an event in space, we have four dimensions.
	\begin{remark}
		Notice that the time axis is not $t$ but $ct$, because the dimension of it should be in length.
	\end{remark}

	% Lecture 2: More on Spacetime and Events; Inertial Frames of reference and Coordinate Systems, Euclidean Metrics, Kinematics: Displacement in a particular Frame. Vectors in a three-dimensional space, in index notation. 

	Thus we have the more formal definition or general statement:
	\begin{claim}
		To specify an event in the universe, we provide four labels:
		\begin{equation}
		  (ct,x,y,z)
		\end{equation}

		where $x,y$ and $z$ are for space or location, and $ct$ as time. Here $c$ is introduced as a speed constant (not "light speed" in its literal sense), which we will cover in due course.
	\end{claim}

	This encourages us to think in the perspective that the "arena" of the universe is a four-dimensional mathematical space that we will call spacetime. A teaser for the mathematical space: "manifolds". This will be covered in higher division courses.

	\vspace{\baselineskip}
	Now we have the empirical facts about space.
	\begin{claim}
		Here are the empirical facts about space:
		\begin{enumerate}
			\item {It is in three dimensions, with an extra dimension of time,}
			\item {It is isotropic, which means it looks the same in every direction,}
			\item {It is homogenous, as in it is the same at every point.}
		\end{enumerate}
	\end{claim}

	In the larger context of general relativity, which includes gravity, there are many space-times. To distinguish ours, the one relevant for special relativity, we will call it Minkowski Spacetime. Thus to summarize, \textit{\textbf{The universe is a four dimensional Minkowski spacetime, with points and events labeled $(ct,x,y,z)$}}.

	\vspace{2\baselineskip}
	\textbf{Observers and inertial frames.} There is a special class of frames of reference are known as inertial frames. It is the result of Newton's first law, which we will cover later in the course. 

	\vspace{1\baselineskip}
	Consider a free particle— a particle subject to no influences. (we assume that this is true in the context of classical mechanics). Then, an inertial frame is a frame in which a free particle moves uniformly in a straight line, or not at all. This implies that if one inertial frame exists, by moving the origin or setting another frame of reference that is moving at constant speed relative to our first, or even performing static rotations, these will all define inertial frames equally.

	\vspace{1\baselineskip}
	A straight line in this sense can be defined as the shortest distance between two points in space— when we talk about geometric spaces, we are talking about the geometry of Euclid. Another remark is that coordinates are entirely artificial, and the choice of coordinates is arbitrary. Thus the laws of physics \textit{cannot} depend on coordinates. This actually has profound consequences: the laws that we write down are not arbitrary and are subject to the principle.

	\vspace{1\baselineskip}
	Take the vector for example. It is defined as a "quantity with a magnitude and a direction", independent of coordinate systems— that is the motivation for vectors, scalars, tensors... and other ideas in physics. 

	\newpage
	\section{Coordinate Systems}
	\textbf{Distance.} To express distances in coordinate systems, we have Pythagorean's Theorem:
	\begin{theorem}
		For two points in space, in a Euclidean coordinate system, we have the distance, $\Delta \ell$ is defined as:
		\begin{equation}
		  \Delta \ell^{2} = \Delta x^{2} + \Delta y^{2} + \Delta z^{2}.
		\end{equation}

		If the quantity is infinitesimal:
		\begin{equation}
		  \dd \ell^{2} = \dd x^{2} + \dd y^{2} + \dd z^{2}.
		\end{equation}
	\end{theorem}

	\begin{remark}
		Note that using the calculus of variations would reveal the straight line as the shortest distance between two arbitrary points.
	\end{remark}

	Henceforth— and unless otherwise stated— we will look in inertial frames. The first we will introduce, of course, is the Cartesian coordinates, where we choose three mutually perpendicular axes. The other two coordinate systems, spherical / cylindrical coordinates are defined well enough in Calculus, with the exception of using $(ct,r,\theta,\varphi)$ where $\theta$ is the azimuthal angle and $\varphi$ is the polar coordinate, different from the mathematical definition.

	\vspace{1\baselineskip}
	Now we introduce the shorthand notation for the axes:
	\begin{equation}
	  (ct,x,y,z) \;\Rightarrow\; (x^{0},x^{1},x^{2},x^{3}).
	\end{equation}

	Or, when in spacetime coordinates we have:
	\begin{equation}
	  x^{\mu}, x^{\nu}, x^{\lambda}, x^{\sigma}, \;\mu,\nu,\lambda,\sigma \in [0,3],
	\end{equation}

	or, when we are in space,
	\begin{equation}
	  x^{i}, x^{j}, x^{m}, x^{k}, \; i,j,m,k \in [1,3].
	\end{equation}

	There will be an abuse of notation that is quite standard:
	\begin{equation}
	  x^{\mu} = (x^{0},x^{1},x^{2},x^{3}).
	\end{equation}

	\vspace{1\baselineskip}
	\textbf{Defining time.} The natural choice of parameterization is to use coordinate time:
	\begin{equation}
	  t \equiv \frac{x^{0}}{c}.
	\end{equation}
	
	Then, we would have
	\begin{equation}
	  x^{0} = x^{0}(t) = ct \quad \Rightarrow \quad x^{\mu} = x^{\mu}(t) = (ct, x^{1}(t),x^{2}(t),x^{3}(t)).
	\end{equation}

	When we use Cartesian coordinates, or $x^{1} = x, x^{2} = y, x^{3} = z$, we would have the positions defined, when we suppress the time factor for simplicity, would be
	\begin{equation}
	  (x(t),y(t),z(t)).
	\end{equation}

	Now we move on to the definition of a "frame", or a "slice" of spacetime, as:
	\begin{definition}
		A frame is a spatial slice of spacetime where every point on the slice is simultaneous with every other.
	\end{definition}

	We can consider this as a "stack of slices", where the progression sequence is the direction of time. But this is inaccurate: it has the underlying assumption that space and time are independent of observers: what is considered simultaneous in the perspective of one observer is not necessarily simultaneous in another. We will come back to this notion in time dilation.

	\vspace{2\baselineskip}
	\textbf{Displacement.} As a first attempt at motion, define the displacement $\Delta x^{i}$ as:
	\begin{equation}
	  \Delta x^{i} \equiv x^{i}_{\mathrm{f}} - x^{i}_{\mathrm{i}}.
	\end{equation}

	Where $x^{i}_{\mathrm{f}} = x^{i}(t_{\mathrm{f}})$ and $x^{i}_{\mathrm{i}} = x^{i}(t_{\mathrm{i}})$, where the duration is $\Delta t = t_{\mathrm{f}}-t_{\mathrm{i}}$. We can immediately see that this is a crude definition of motion because it considers only the endpoint and starting point, providing just the "Line of Sight" distance between two points. So, to further clarify, we will define also its magnitude:
	\begin{equation}
	  \vqty{\Delta x^{i}} \equiv \sqrt{(\Delta x)^{2} + (\Delta y)^{2} + (\Delta z)^{2}}.
	\end{equation} 

	We notice that this is an extremely crude way still, because it does not count for any motion of the particle in between the time of motion. However if we take the limit, this becomes extremely useful. Let's first obtain this from the Euclidean metric. 
	\begin{proof}
		We first introduce a parameter, $\lambda$, where the line is parameterized by
		\begin{equation}
		  x(\lambda) = a \lambda, \; y(\lambda) = b \lambda, \; z(\lambda) = c \lambda.
		\end{equation}

		With $0 \le \lambda le 1$. Let
		\begin{equation}
		  L = \vqty{\Delta x^{i}},
		\end{equation}

		This, when we start with a line assuming $c=0$, we get $y = \frac{b}{a} x.$ When we take the length differential,
		\begin{equation}
		  \dd \ell = \sqrt{\Delta x^{2} + \Delta y^{2} + \Delta z^{2}},
		\end{equation}

		the length would be:
		\begin{equation}
		  L = \int \dd \ell = \int_{0}^{1} \sqrt{\Delta x^{2} + \Delta y^{2} + \Delta z^{2}} \,\dd \lambda = \sqrt{a^{2}+b^{2}+c^{2}}.
		\end{equation}

		For this context is is a bit of an overkill, but for spacetime the incorporation of time would lead to more counterintuitive answers.
	\end{proof}

	\section{Vectors in Three-Dimensional Euclidean Space}
	From the definition of displacement we notice that it is correlated with direction- it has both a magnitude, and a direction. Thus we introduce the idea of vectors:
	\begin{definition}
		A quantity with only magnitude is a scalar. A quantity with magnitude and direction is a vector. A vector can thus be denoted as a directed line segment, i.e. an arrow. Note that vectors are independent of coordinates.
	\end{definition}

	Now we go on to define the algebra, transformation, and calculus of vectors. We will start with notation. Vectors will be noted as $\overrightarrow{A}$ or $\mathbf{A}$, with its magnitude $\vqty{\overrightarrow{A}}$ or $\mathrm{A}$. For the direction of the vector we wil denote as the unit vector $\mathbf{\hat{A}}$.

	\vspace{1\baselineskip}
	In Cartesian coordinates this course will prefer $\{\hat{\mathbf{x}}, \hat{\mathbf{y}}, \hat{\mathbf{z}}\}$. But there is a better and more general way to note this:
	\begin{equation}
	  \{\hat{\mathbf{e}}_1,\hat{\mathbf{e}}_2,\hat{\mathbf{e}}_3\}.
	\end{equation}

	This extends to the claim:
	\begin{claim}
		Any arbitrary vector $\hat{\mathbf{A}}$ can be expressed as a linear combination of the basis vectors $\left\{ \hat{e_{i}} \right\} $.
	\end{claim}

	\vspace{1\baselineskip}
	\textbf{Vector addition.} Geometrically we define the diagonal of the parallelogram formed by the two vectors to be added. We also define multiplication by a scalar:
	\begin{definition}
	  Let $\alpha \in \mathbb{R}$ and $\hat{\mathbf{e}}_{i}$ as the basis vectors. Define $\alpha \mathbf{e}^{i}$ by:
	  \begin{equation}
		\vqty{\alpha \hat{\mathbf{e}}_{i}} = \vqty{\alpha}.
	  \end{equation}

	  If $\alpha>0$ the result is parallel to the original vector, if $\alpha<0$ is antiparallel to the original vector, if $\alpha=0$ the result is a zero vector.
	\end{definition}
	
	We also define, formally, vector addition as the diagonal of the parallelogram formed by the two vector components. This is important because we can now express any vector using these three base vectors:
	\begin{equation}
	  \mathbf{A} = A^{1}\hat{\mathbf{e}}_1 + A^{2}\hat{\mathbf{e}}_2 + A^{3}\hat{\mathbf{e}}_3.
	\end{equation}

	Where the components of $\mathbf{A}$ are $\{A^{1},A^{2},A^{3}\}$, and they can be zero or nonzero. Here the components are in superscript because in abstract algebra (and by extension four-dimensional space), these will represent component vectors. The magnitude of $\mathbf{A}$ is given by
	\begin{equation}
	  \vqty{\mathbf{A}} \equiv A = \sqrt{\left( A^{1} \right)^{2} + \left( A^{2} \right)^{2} + \left( A^{3} \right)^{2}}.
	\end{equation}

	We can now write, for two arbitrary vectors $\mathbf{A}$ and $\mathbf{B}$, and scalars, $\alpha$ and $\beta$, we can write the following:
	\begin{definition}
		Linear combinations:
		\begin{equation}
		  \begin{aligned}
		  \alpha \mathbf{A} + \beta \mathbf{B} = (\alpha A^{1} + \beta B^{1})\hat{\mathbf{e}}_1 + (\alpha A^{2} + \beta B^{2})\hat{\mathbf{e}}_2 + (\alpha A^{3} + \beta B^{3})\hat{\mathbf{e}}_3. \\
		  \end{aligned}
		\end{equation}

		*Note that we do not need to provide definitions for subtraction because $\mathbf{A}-\mathbf{B} = \mathbf{A} + (-\mathbf{B})$. There is, though, a useful way of drawing the geometric- the final vector minus the initial vector, linking the tips of $\mathbf{A}$ and $\mathbf{B}$.
	\end{definition}

	This sounds like a mouthful and is complicated to write. Usually we write out the sigma in a compact form, with $i$ from $1$ through $3$, but it is still too difficult. Note, though, that $i$ is a \textit{dummy variable}, meaning that within the summation the variables can be reassigned for other letters. They are essentially placeholders for numbers, so in any sum we can freely change the letter $i$. But, if the variables are not summed (when they are \textit{free variables}), they are not interchangeable. So. with that in mind, we introduce \textbf{The Einstein Summation Convention.} We are writing the following:
	\begin{claim}
		We write the vector $\mathbf{A}$ as:
		\begin{equation}
		  \mathbf{A} = A^{1}\hat{\mathbf{e}}_1 + A^{2}\hat{\mathbf{e}}_2 + A^{3}\hat{\mathbf{e}}_3 = \sum_{i=1}^3 A^{i}\hat{\mathbf{e}}_{i} \equiv A^{i}\hat{\mathbf{e}}_{i}.
		\end{equation}
	\end{claim}

	Essentially we omit the sigma. Is should be one superscript and one subscript, however in Euclidean space ($\mathbb{E}^{3}$) this does not matter; thus we could write $A_{i}\hat{\mathbf{e}}_{i}$. However in the Minkowski space this is important. Now we introduce an example:
	\begin{example}
		Write the following using the summation convention:
		\begin{enumerate}
			\item {$\mathbf{A} \pm \mathbf{B}$,}
			\item {$\alpha \mathbf{A}$.}
		\end{enumerate}

		\vspace{1\baselineskip}
		\textbf{Solution.} (a)The first one is easily deducted:
		\begin{equation}
		  \alpha \mathbf{A} + \beta \mathbf{B} = (\alpha A^{1} + \beta B^{1})\hat{\mathbf{e}}_1 + (\alpha A^{2} + \beta B^{2})\hat{\mathbf{e}}_2 + (\alpha A^{3} + \beta B^{3})\hat{\mathbf{e}}_3
		\end{equation}

		Writing out the summation and dropping the sigma:
		\begin{equation}
		  \mathbf{A} \pm \mathbf{B} = (A^{i}\pm B^{i})\hat{\mathbf{e}}_{i}.
		\end{equation}

		(b) the second one we already did:
		\begin{equation}
		  \alpha \mathbf{A} = \alpha A^{i}\hat{\mathbf{e}}_{i}.
		\end{equation}
	\end{example}

	Now an exercise question:
	\begin{exercise}
		Write $A$ and $\hat{\mathbf{A}}$ using the summation convention. 
		
		\vspace{1\baselineskip}
		\textbf{Solution.} We first write out the intermediate equation:
		\begin{equation}
		  A = \sqrt{\left( A^{1} \right)^{2} + \left( A^{2} \right)^{2} + \left( A^{3} \right)^{2}},
		\end{equation}

		and it would change into (for clarity we don't put extra superscripts):
		\begin{equation} \label{eq0411}
		  A = \sqrt{A^{i}A^{i}}.
		\end{equation}

		And the unit vector $\hat{\mathbf{A}}$ would be easy:
		\begin{equation}
		  \hat{\mathbf{A}} = \frac{A^{i}\hat{\mathbf{e}}_{i}}{\sqrt{A^{i}A^{i}}}.
		\end{equation}
	\end{exercise}

	\textbf{Scalar products / Dot products.} We define an operation (the "product") that multiplies vectors and gives scalars. This is noticed as $\mathbf{A} \cdot \mathbf{B}$. It is defined as:
	\begin{definition}
		The dot product is defined as:
		\begin{equation}
		  \mathbf{A} \cdot \mathbf{B} = AB\cos \theta,
		\end{equation}
		where $\theta$ is the smallest angle between $A$ and $B$. Note that it is symmetric and communicative, where $\mathbf{A} \cdot \mathbf{B} = \mathbf{B} \cdot \mathbf{A}$.
	\end{definition}

	This introduces us to the Kronecker Delta:
	\begin{equation}
	  \hat{\mathbf{e}}_{i} \cdot \hat{\mathbf{e}}_{j} = \delta_{ij},
	\end{equation}
	
	where the delta is a compact form of the identity matrix.
	\begin{equation}
	  \delta_{ij} = \begin{cases} 1, \quad \text{if} \; i=j, \\[2mm] 0 \quad \text{if} \; i \neq j.  \end{cases}
	\end{equation}

	This means we have an orthonormal basis. Now we prove that $\mathbf{A} \cdot \mathbf{B} = A^{i}B^{i}$. 
	\begin{proof}
		We use the Kronecker Delta:
		\begin{equation}
		  \begin{aligned}
		  \mathbf{A} \cdot \mathbf{B} &= (A^{i}\hat{\mathbf{e}}_{i}) \cdot (B^{j}\hat{\mathbf{e}}_{j}) = A^{i}B^{j}\delta_{ij} \\[2mm]
		  &= \sum_{i=1}^{3} \sum_{j=1}{3} A^{i}B^{j}\delta_{ij} \\[2mm]
		  &= A^{i}B^{i}.
		  \end{aligned} 
		\end{equation}
	\end{proof}

	Now we go for an exercise:
	\begin{exercise}
		Show that:
		\begin{enumerate}
			\item {$\mathbf{A} \cdot \mathbf{A} = A^{i}A^{i} = A^{2},$}
			\item {$\hat{\mathbf{e}}_{i} \cdot \mathbf{A} = A^{i}.$}
		\end{enumerate}

		\vspace{1\baselineskip}
		\textbf{Solution.} (a) For the first equation, we simply plug in $\mathbf{A}$ for the general definition above and have:
		\begin{equation}
		  \mathbf{A} \cdot \mathbf{A} = A^{i}A^{i}.
		\end{equation}

		And, because we know, from Equation \ref{eq0411},
		\begin{equation}
		  A = \sqrt{A^{i}A^{i}} \quad \Rightarrow \quad A^{2} = A^{i}A^{i},
		\end{equation}

		we can prove the original equation.

		\vspace{1\baselineskip}
		(b) We expand the equation:
		\begin{equation}
		  \hat{\mathbf{e}}_{i} \cdot \mathbf{A} = \hat{\mathbf{e}}_{i} \cdot (A^{j}\hat{\mathbf{e}}_{j}) = A^{j}\delta_{ij} = A^{i}.
		\end{equation}
	\end{exercise}

	Another example here: we know that the Euclidean line element
	\begin{equation}
	  \dd \ell ^{2} = \dd x^{2} + \dd y^{2} + \dd z^{2}.
	\end{equation}

	\begin{example}
		Show that:
		\begin{equation}
		  \dd \ell^{2} = \delta_{ij} \dd x^{i} \dd x^{j}.
		\end{equation}

		\vspace{1\baselineskip}
		\textbf{Solution.} We have:
		\begin{equation}
		  \begin{aligned}
		  \dd \ell^{2} &= \dd x^{2} + \dd y^{2} + \dd z^{2} \\
		  &= (\dd x^{1})^{2} + (\dd x^{2})^{2} + (\dd x^{3})^{2} \\
		  &= \sum_{i=1}^{3} (\dd x^{i})^{2} = \sum_{i=1}^{3} \dd x^{i} \dd x^{i} \\
		  &= \sum_{i=1}^{3} \sum_{j=1}^{3} \delta_{ij} \dd x^{i} \dd x^{j}.
		  \end{aligned}
		\end{equation}

		Drop the summation and we get the result.
	\end{example}

	An extension of this example is this: Suppose
	\begin{equation}
	  \dd s^{2} = g_{\mu \nu} (x) \dd x^{\mu} \dd x^{\nu},
	\end{equation}

	solving for $g$ gives you the function for gravity, which is the definition for "geometry of space". Also when we define
	\begin{equation}
	  \dd s^{2} \equiv \left( 1-\frac{2M}{r} \right) c\,\dd t^{2} + \left( 1-\frac{2M}{r} \right)^{-1}\,\dd r^{2} + r^{2}\sin ^{2}\theta \,\dd\Omega^{2},
	\end{equation}

	we can determine the event horizon of a black hole because when $r=2M$, the second differential gets to infinity.

	\vspace{1\baselineskip}
	We now define the \textbf{Cross product}. 
	\begin{definition}
		Multiply two vectors and get a vector through $\mathbf{A} \times \mathbf{B}$. We have:
		\begin{equation}
		  \mathbf{A} \times \mathbf{B} = AB\sin \theta \hat{\mathbf{n}},
		\end{equation}

		where the direction of $\hat{\mathbf{n}}$ is defined by the right hand rule. Note that this is anti-symmetric product (thus evidently not associative), because by the right hand rule,
		\begin{equation}
		  \mathbf{A} \times \mathbf{B} = -\mathbf{B} \times \mathbf{A}.
		\end{equation}
	\end{definition}

	Now we need to write down the cross product of the basis vectors. We have:
	\begin{equation}
	  \hat{\mathbf{e}}_{i} \times \hat{\mathbf{e}}_{j} = \epsilon_{ijk} \hat{\mathbf{e}}_{k}.
	\end{equation}

	The $\epsilon$ is defined as the Levi-Civita symbol, where
	\begin{equation}
	  \epsilon_{ijk} = \begin{cases} 1, \quad \text{if} \; \{i,j,k\} \; \text{is an even permutation of} \; \{1,2,3\}, \\ 1, \quad \text{if} \; \{i,j,k\} \; \text{is an odd permutation of} \; \{1,2,3\}, \\ 0, \quad \text{otherwise.}  \end{cases}
	\end{equation}

	Note that $k$ is a free variable. The exercise now is to expand the vector multiplication of the base vectors.
	\begin{exercise}
		The first one is
		\begin{equation}
		  \hat{\mathbf{e}}_1 \times \hat{\mathbf{e}}_2 = \epsilon_{121}\hat{\mathbf{e}}_1 + \epsilon_{122}\hat{\mathbf{e}}_2 + \epsilon_{123}\hat{\mathbf{e}}_3 = \epsilon_{123} \hat{\mathbf{e}}_3 = \hat{\mathbf{e}}_3,
		\end{equation}

		The second one is:
		\begin{equation}
		  \hat{\mathbf{e}}_2 \times \hat{\mathbf{e}}_3 = \epsilon_{231}\hat{\mathbf{e}}_1 + \epsilon_{232}\hat{\mathbf{e}}_2 + \epsilon_{233}\hat{\mathbf{e}}_3 = \epsilon_{231}\hat{\mathbf{e}}_1 = \hat{\mathbf{e}}_1,
		\end{equation}

		And the last one is:
		\begin{equation}
		  \hat{\mathbf{e}}_1 \times \hat{\mathbf{e}}_3 = \epsilon_{132} \hat{\mathbf{e}}_2 = -\hat{\mathbf{e}}_2.
		\end{equation}
	\end{exercise}

	Now consider two arbitrary vectors $\mathbf{A}$ and $\mathbf{B}$. We have:
	\begin{equation}
	  \begin{aligned}
	  \mathbf{A} \times \mathbf{B} &= (A^{i}\hat{\mathbf{e}}_{i}) \times (B^{i}\hat{\mathbf{e}}_{j}) \\
	  =& A^{i}B^{j}(\hat{\mathbf{e}}_{i} \times \hat{\mathbf{e}}_{j}) = A^{i}b^{j}(\epsilon_{ijk}\hat{\mathbf{e}}_{k}) \\
	  =& \epsilon_{ijk} A^{i}b^{j}\hat{\mathbf{e}}_{k} \\
	  =& \epsilon_{123}A^{1}B^{2}\hat{\mathbf{e}}_3 + \epsilon_{132}A^{1}B^{3}e^{2} + \epsilon_{213}A^{2}B^{1}\hat{\mathbf{e}}_3 \\
	  &+ \epsilon_{231}A^{2}B^{3}\hat{\mathbf{e}}_1 + \epsilon_{312}A^{3}B^{1}\hat{\mathbf{e}}_2 + \epsilon_{321}A^{3}B^{2}\hat{\mathbf{e}}_1 \\
	  =& (A^{2}B^{3}-A^{3}B^{2})\hat{\mathbf{e}}_1 + (A^{3}B^{1}-A^{1}B^{3})\hat{\mathbf{e}}_2 + (A^{1}b^{2}-A^{2}B^{1})\hat{\mathbf{e}}_3.
	  \end{aligned}
	\end{equation}

	This is cleaner than the traditional determinant format because it is reproducible in higher dimensions. Note though, that only the Levi-Civita symbol can be defined, and in four dimensions or more, we cannot define the cross product comfortably because there is one more dimension of ambiguity. Now we have an example.
	\begin{example}
		Show that the identity holds:
		\begin{equation}
		  \epsilon_{ijk}\epsilon_{kmn} = \delta_{im}\delta_{jn} - \delta_{in}\delta_{jm}.
		\end{equation}

		We know that for this the single free variable is $k$.
	\end{example}

	Also: Prove the identity 
	\begin{equation}
	  \mathbf{A} \times (\mathbf{B} \times \mathbf{C}) = (\mathbf{A} \cdot \mathbf{C})\mathbf{B} - (\mathbf{A} \cdot \mathbf{B})\mathbf{C}.
	\end{equation}

\end{document}